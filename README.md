Как запустить ETL вручную:

  Потрібно завантажити три модулі "extract", "transform", "load", а також файл "trades.csv" на ваш локальний комп'ютер.
  Переконатися, що на вашому локальному комп'ютері встановлено Python та редактор (наприклад, PyCharm).
  Запустити вищезгадані модулі у редакторі один за одним.
  Як результат, отримаємо агреговані дані, завантажені до таблиці "agg_trades_weekly" у базі даних "agg_result.db".
  
  Для того, щоб запустити ETL вручну, перейдіть за посиланням: [https://github.com/maksymche/1/actions].
  У вікні, що відкриється, ліворуч ви побачите розділ "Actions", де знайдете "ETL Pipeline", на який потрібну натиснути.
  Після цього у правій частині вікна з'явиться "Run workflow", куди потрібно буде натиснути, після чого відкриється мікро-вікно, у якому слід обрати ідентичну за назвою кнопку.
  Пайплайн запущено.





  Как работает CI/CD:
  У цьому проєкті CI/CD налаштовано через GitHub Actions.
  GitHub сам запускає ваш ETL-код за певних умов:
    При кожному push - коли ви завантажуєте нові зміни в репозиторій.
    При запуску вручну через кнопку “Run workflow” (workflow_dispatch).
  GitHub запускає ваш ETL та автоматично завантажує результат до бази даних.




  Как бы вы адаптировали решение под 100+ млн строк:
■ Какие технологии замените/добавите?
    Pandas не дуже добре масштабується на сотні мільйонів рядків, тому що висока пам’ять і повільна обробка.
    SQLite та локальні Excel-файли не підходять для великих обсягів.
    Я б замінив Pandas на Dask, Polars або PySpark.
    SQLite - на PostgreSQL, ClickHouse, або BigQuery.
    Excel/CSV - на Parquet або Feather.

■ Какую архитектуру ETL предложите?

  Extract: зчитування даних із джерела (CSV, бази, API).
    Для великих даних — потокове або блокове зчитування, щоб не завантажувати всю таблицю в пам’ять.

  Transform: очищення даних, конвертація дат, обчислення агрегацій.
    Для великих обсягів — використання Dask, Polars або PySpark для розподіленої обробки.
 
  Load: збереження результатів у базу даних (PostgreSQL, ClickHouse) або у колоночно-орієнтовані файли (Parquet).
    Для аналітики та подальших звітів.

  Автоматизація:
    Використання CI/CD (GitHub Actions, Airflow або Prefect) для регулярного виконання ETL.
    Логи та моніторинг виконання (кількість рядків, час виконання, помилки).

■ Какие метрики мониторинга ETL вы бы внедрили?
 
  Продуктивність
    Час виконання кожного етапу ETL (Extract / Transform / Load).
    Швидкість обробки рядків (rows/sec).
 
  Якість даних
    Кількість пропущених або некоректних значень після етапу трансформації.
    Кількість дублікатів (якщо критично для бізнес-логіки).
 
  Надійність
    Кількість успішних/неуспішних запусків workflow.
    Помилки на етапі Extract / Transform / Load.
  
  Результат
    Кількість рядків у агрегованій таблиці (agg_trades_weekly).
    Сумарний обсяг даних (total_volume, total_pnl) для перевірки на відповідність очікуванням.
  
  Логування
    Збереження логів виконання для швидкого відстеження проблем.
    Можливість оповіщення при падінні пайплайну (email, Slack, Teams).
    

■ Где будут храниться входные и выходные данные?

  Вхідні дані
    Файл trades.csv зберігається в репозиторії поруч із скриптами ETL.
    Для масштабування (100+ млн рядків) використовуватиму S3, Google Cloud Storage або інші об’єктні сховища, щоб не тримати гігантські файли локально.

  Вихідні дані
    База даних SQLite (agg_result.db) — локальна база для швидкої роботи та тестів.
    xlsx/csv — для звітності та аналізу.
    Для великих даних: використовуватиму колоночно-орієнтовані формати(Parquet) або аналітичну базу(PostgreSQL, ClickHouse, BigQuery).
